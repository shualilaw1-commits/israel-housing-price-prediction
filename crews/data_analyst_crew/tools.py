"""Tools for Data Analyst Crew - ×›×œ×™× ×œ×¦×•×•×ª ×× ×ª×—×™ ×”× ×ª×•× ×™×"""
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
# Removed: from sklearn.datasets import fetch_california_housing
# Now using Israel housing dataset
from crewai.tools import BaseTool
from typing import Type, Any
from pydantic import BaseModel, Field
from datetime import datetime


class DataIngestionInput(BaseModel):
    """Input schema for Data Ingestion Tool"""
    output_dir: str = Field(default="outputs", description="Directory to save outputs")


class DataIngestionTool(BaseTool):
    name: str = "Data Ingestion Tool"
    description: str = "×˜×•×¢×Ÿ ××ª Israel Housing dataset ×•×©×•××¨ ××•×ª×•"

    def _run(self, output_dir: str = "outputs") -> str:
        """×˜×•×¢×Ÿ ××ª ×”× ×ª×•× ×™× ×•×™×•×¦×¨ dataset contract"""
        try:
            # ×™×¦×™×¨×ª ×ª×™×§×™×™×” ×× ×œ× ×§×™×™××ª
            os.makedirs(output_dir, exist_ok=True)

            # ×˜×¢×™× ×ª ×”× ×ª×•× ×™× ××™×©×¨××œ
            # × ×‘×“×•×§ ×× ×§×™×™× ×§×•×‘×¥ raw_data.csv, ×× ×œ× - × ×˜×¢×Ÿ ××”× ×ª×™×‘ ×”×™×—×¡×™
            raw_data_path = os.path.join(output_dir, "raw_data.csv")
            
            # ×× ×”×§×•×‘×¥ ×œ× ×§×™×™×, × × ×¡×” ×œ×˜×¢×•×Ÿ ××”×ª×™×§×™×™×” ×”×¨××©×™×ª
            if not os.path.exists(raw_data_path):
                # × × ×¡×” ×œ×˜×¢×•×Ÿ ××”×ª×™×§×™×™×” ×”×¨××©×™×ª (×× ×”×¡×§×¨×™×¤×˜ create_israel_dataset.py ×›×‘×¨ ×¨×¥)
                parent_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
                possible_paths = [
                    os.path.join(parent_dir, "create_israel_dataset.py"),
                    os.path.join(output_dir, "raw_data.csv"),
                    "outputs/raw_data.csv"
                ]
                
                # ×× ××™×Ÿ × ×ª×•× ×™×, × ×¦×˜×¨×š ×œ×”×¨×™×¥ ××ª create_israel_dataset.py
                return "âŒ ×§×•×‘×¥ raw_data.csv ×œ× × ××¦×. ×× × ×”×¨×¥ ×ª×—×™×œ×”: python create_israel_dataset.py"
            
            # ×˜×¢×™× ×ª ×”× ×ª×•× ×™×
            df = pd.read_csv(raw_data_path, encoding='utf-8')

            # ×™×¦×™×¨×ª dataset contract
            contract = {
                "dataset_name": "Israel Housing Dataset",
                "source": "Generated dataset based on Israeli real estate market",
                "load_date": datetime.now().isoformat(),
                "num_rows": len(df),
                "num_columns": len(df.columns),
                "columns": {
                    col: {
                        "dtype": str(df[col].dtype),
                        "null_count": int(df[col].isnull().sum()),
                        "unique_count": int(df[col].nunique())
                    }
                    for col in df.columns
                },
                "description": "Dataset ×©×œ ×“×™×¨×•×ª ×‘×™×©×¨××œ ×¢× ××™×“×¢ ×¢×œ ×¢×¨×™×, ×’×•×“×œ, ×—×“×¨×™×, ××™×§×•× ×•××—×™×¨×™×",
                "target": "Price_Millions"
            }

            # ×©××™×¨×ª ×”×—×•×–×”
            contract_path = os.path.join(output_dir, "dataset_contract.json")
            with open(contract_path, 'w', encoding='utf-8') as f:
                json.dump(contract, f, ensure_ascii=False, indent=2)

            return f"âœ“ ×˜×¢×™× ×ª × ×ª×•× ×™× ×”×¦×œ×™×—×”!\n" \
                   f"- ×©×•×¨×•×ª: {len(df):,}\n" \
                   f"- ×¢××•×“×•×ª: {len(df.columns)}\n" \
                   f"- ×§×‘×¦×™× × ×©××¨×• ×‘: {output_dir}"

        except Exception as e:
            return f"âŒ ×©×’×™××” ×‘×˜×¢×™× ×ª ×”× ×ª×•× ×™×: {str(e)}"


class DataCleaningInput(BaseModel):
    """Input schema for Data Cleaning Tool"""
    input_file: str = Field(default="outputs/raw_data.csv")
    output_dir: str = Field(default="outputs")


class DataCleaningTool(BaseTool):
    name: str = "Data Cleaning Tool"
    description: str = "×× ×§×” ××ª ×”× ×ª×•× ×™× ×•××˜×¤×œ ×‘×¢×¨×›×™× ×—×¡×¨×™× ×•×—×¨×™×’×™×"

    def _run(self, input_file: str = "outputs/raw_data.csv", output_dir: str = "outputs") -> str:
        """×× ×§×” ××ª ×”× ×ª×•× ×™×"""
        try:
            # ×§×¨×™××ª ×”× ×ª×•× ×™×
            df = pd.read_csv(input_file)
            original_rows = len(df)

            # 1. ×‘×“×™×§×ª ×¢×¨×›×™× ×—×¡×¨×™×
            missing_values = df.isnull().sum()
            missing_report = missing_values[missing_values > 0].to_dict()

            # 2. ×˜×™×¤×•×œ ×‘-inf values
            df.replace([np.inf, -np.inf], np.nan, inplace=True)

            # 3. ×–×™×”×•×™ outliers ×¢× IQR method
            outliers_count = {}
            for col in df.select_dtypes(include=[np.number]).columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()
                if outliers > 0:
                    outliers_count[col] = int(outliers)

            # 4. ××—×™×§×ª ×©×•×¨×•×ª ×¢× ×¢×¨×›×™× ×—×¡×¨×™× (×× ×™×©)
            df.dropna(inplace=True)

            # 5. ×©××™×¨×ª ×”× ×ª×•× ×™× ×”×× ×•×§×™×
            clean_data_path = os.path.join(output_dir, "clean_data.csv")
            df.to_csv(clean_data_path, index=False)

            # 6. ×¢×“×›×•×Ÿ dataset contract (×× ×§×™×™×)
            contract_path = os.path.join(output_dir, "dataset_contract.json")
            if os.path.exists(contract_path):
                try:
                    with open(contract_path, 'r', encoding='utf-8') as f:
                        contract = json.load(f)
                    
                    contract["cleaning"] = {
                        "cleaning_date": datetime.now().isoformat(),
                        "original_rows": original_rows,
                        "cleaned_rows": len(df),
                        "rows_removed": original_rows - len(df),
                        "missing_values_found": missing_report,
                        "outliers_detected": outliers_count,
                        "cleaning_actions": [
                            "×”×¡×¨×ª ×¢×¨×›×™ inf",
                            "××—×™×§×ª ×©×•×¨×•×ª ×¢× ×¢×¨×›×™× ×—×¡×¨×™×",
                            "×–×•×”×• outliers ××š × ×©××¨×• (××•××œ×¥ ×œ×‘×“×•×§)"
                        ]
                    }

                    with open(contract_path, 'w', encoding='utf-8') as f:
                        json.dump(contract, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    pass  # ×× ×™×© ×‘×¢×™×” ×‘×¢×“×›×•×Ÿ ×”×—×•×–×”, × ××©×™×š

            return f"âœ“ × ×™×§×•×™ × ×ª×•× ×™× ×”×•×©×œ×!\n" \
                   f"- ×©×•×¨×•×ª ××§×•×¨×™×•×ª: {original_rows:,}\n" \
                   f"- ×©×•×¨×•×ª ×œ××—×¨ × ×™×§×•×™: {len(df):,}\n" \
                   f"- ×©×•×¨×•×ª ×©×”×•×¡×¨×•: {original_rows - len(df):,}\n" \
                   f"- Outliers ×©×–×•×”×•: {sum(outliers_count.values())}"

        except Exception as e:
            return f"âŒ ×©×’×™××” ×‘× ×™×§×•×™ ×”× ×ª×•× ×™×: {str(e)}"


class EDAInput(BaseModel):
    """Input schema for EDA Tools"""
    input_file: str = Field(default="outputs/clean_data.csv")
    output_dir: str = Field(default="outputs")


class DistributionAnalysisTool(BaseTool):
    name: str = "Distribution Analysis Tool"
    description: str = "×™×•×¦×¨ × ×™×ª×•×— ×”×ª×¤×œ×’×•×™×•×ª ×¢× ×”×™×¡×˜×•×’×¨××•×ª ×•-box plots"

    def _run(self, input_file: str = "outputs/clean_data.csv", output_dir: str = "outputs") -> str:
        try:
            df = pd.read_csv(input_file)
            fig_dir = os.path.join(output_dir, "figures")
            os.makedirs(fig_dir, exist_ok=True)

            # ×™×¦×™×¨×ª figure ×¢× subplots
            num_cols = len(df.columns)
            fig, axes = plt.subplots(num_cols, 2, figsize=(15, num_cols * 4))

            for idx, col in enumerate(df.columns):
                # Histogram
                axes[idx, 0].hist(df[col], bins=50, edgecolor='black', alpha=0.7)
                axes[idx, 0].set_title(f'Distribution of {col}')
                axes[idx, 0].set_xlabel(col)
                axes[idx, 0].set_ylabel('Frequency')

                # Box plot
                axes[idx, 1].boxplot(df[col])
                axes[idx, 1].set_title(f'Box Plot of {col}')
                axes[idx, 1].set_ylabel(col)

            plt.tight_layout()
            plt.savefig(os.path.join(fig_dir, "distributions.png"), dpi=300, bbox_inches='tight')
            plt.close()

            return "âœ“ × ×™×ª×•×— ×”×ª×¤×œ×’×•×™×•×ª × ×•×¦×¨ ×‘×”×¦×œ×—×”"

        except Exception as e:
            return f"âŒ ×©×’×™××”: {str(e)}"


class CorrelationAnalysisTool(BaseTool):
    name: str = "Correlation Analysis Tool"
    description: str = "×™×•×¦×¨ ××˜×¨×™×¦×ª ×§×•×¨×œ×¦×™×•×ª ×•-pairplot"

    def _run(self, input_file: str = "outputs/clean_data.csv", output_dir: str = "outputs") -> str:
        try:
            df = pd.read_csv(input_file)
            fig_dir = os.path.join(output_dir, "figures")
            os.makedirs(fig_dir, exist_ok=True)

            # Correlation heatmap
            plt.figure(figsize=(12, 10))
            corr_matrix = df.corr()
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                       square=True, linewidths=1, cbar_kws={"shrink": 0.8})
            plt.title('Correlation Matrix', fontsize=16)
            plt.tight_layout()
            plt.savefig(os.path.join(fig_dir, "correlation_heatmap.png"), dpi=300, bbox_inches='tight')
            plt.close()

            # Pairplot ××™× ×˜×¨××§×˜×™×‘×™ ×¢× plotly
            fig = px.scatter_matrix(df, dimensions=df.columns[:5],  # 5 ××©×ª× ×™× ×¨××©×•× ×™×
                                   title="Pairplot - Interactive")
            fig.write_html(os.path.join(fig_dir, "pairplot.html"))

            return "âœ“ × ×™×ª×•×— ×§×•×¨×œ×¦×™×•×ª × ×•×¦×¨ ×‘×”×¦×œ×—×”"

        except Exception as e:
            return f"âŒ ×©×’×™××”: {str(e)}"


class GeographicAnalysisTool(BaseTool):
    name: str = "Geographic Analysis Tool"
    description: str = "×™×•×¦×¨ × ×™×ª×•×— ×’×™××•×’×¨×¤×™ ×©×œ ×”× ×ª×•× ×™×"

    def _run(self, input_file: str = "outputs/clean_data.csv", output_dir: str = "outputs") -> str:
        try:
            df = pd.read_csv(input_file)
            fig_dir = os.path.join(output_dir, "figures")
            os.makedirs(fig_dir, exist_ok=True)

            # Geographic scatter plot
            fig = px.scatter(df, x='Longitude', y='Latitude',
                           color='Price_Millions', size='Population',
                           hover_data=['Rooms', 'Size_sqm'],
                           title='Israel Housing - Geographic Distribution',
                           color_continuous_scale='Viridis')

            fig.update_layout(width=1000, height=800)
            fig.write_html(os.path.join(fig_dir, "geographic_analysis.html"))

            return "âœ“ × ×™×ª×•×— ×’×™××•×’×¨×¤×™ × ×•×¦×¨ ×‘×”×¦×œ×—×”"

        except Exception as e:
            return f"âŒ ×©×’×™××”: {str(e)}"


class InsightsGeneratorTool(BaseTool):
    name: str = "Insights Generator Tool"
    description: str = "××¤×™×§ ×ª×•×‘× ×•×ª ××”× ×ª×•× ×™× ×•×©×•××¨ ×‘-markdown"

    def _run(self, input_file: str = "outputs/clean_data.csv", output_dir: str = "outputs") -> str:
        try:
            df = pd.read_csv(input_file)

            # ×—×™×©×•×‘ ×§×•×¨×œ×¦×™×•×ª ×¢× ×”××©×ª× ×” ×”×™×¢×“
            target_corr = df.corr()['Price_Millions'].sort_values(ascending=False)

            # ×™×¦×™×¨×ª ×ª×•×‘× ×•×ª
            insights = f"""# ×ª×•×‘× ×•×ª ×× ×™×ª×•×— ×”× ×ª×•× ×™× - Israel Housing

## ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×›×œ×œ×™×•×ª
- **×¡×š ×©×•×¨×•×ª**: {len(df):,}
- **×¡×š ×¢××•×“×•×ª**: {len(df.columns)}
- **××©×ª× ×” ×™×¢×“**: Price_Millions (××—×™×¨ ×‘××™×œ×™×•× ×™ ×©×§×œ×™×)

## ğŸ¯ ×§×•×¨×œ×¦×™×•×ª ×—×©×•×‘×•×ª
×”××©×ª× ×™× ×”×›×™ ×§×©×•×¨×™× ×œ××—×™×¨ ×”×“×™×¨×”:
{chr(10).join([f'- **{col}**: {corr:.3f}' for col, corr in target_corr.items() if col != 'Price_Millions'])}

## ğŸ’¡ ×ª×•×‘× ×•×ª ××¨×›×–×™×•×ª

### 1. ××©×ª× ×” ×”×›×™ ××©×¤×™×¢
×”××©×ª× ×” **{target_corr.index[1]}** ×”×•× ×”×›×™ ×§×©×•×¨ ×œ××—×™×¨ ×¢× ×§×•×¨×œ×¦×™×” ×©×œ {target_corr.iloc[1]:.3f}

### 2. ×××¤×™×™× ×™ ×”×ª×¤×œ×’×•×ª
- ××—×™×¨ ×××•×¦×¢: {df['Price_Millions'].mean():.2f} ××™×œ×™×•×Ÿ ×©"×—
- ××—×™×¨ ×—×¦×™×•× ×™: {df['Price_Millions'].median():.2f} ××™×œ×™×•×Ÿ ×©"×—
- ×¡×˜×™×™×ª ×ª×§×Ÿ: {df['Price_Millions'].std():.2f} ××™×œ×™×•×Ÿ ×©"×—

### 3. ×“×¤×•×¡×™× ×’×™××•×’×¨×¤×™×™×
- ×”×ª×¤×œ×’×•×ª ×’×™××•×’×¨×¤×™×ª ××’×•×•× ×ª ×‘×™×©×¨××œ
- ××—×™×¨×™× ×’×‘×•×”×™× ×™×•×ª×¨ ×‘××–×•×¨×™× ××¡×•×™××™× (× ×¨××” ×‘-scatter plot)
- ×¢×¨×™× ××¨×›×–×™×•×ª ×›××• ×ª×œ ××‘×™×‘ ×•×¨××ª ×’×Ÿ × ×•×˜×•×ª ×œ×”×™×•×ª ×™×§×¨×•×ª ×™×•×ª×¨

## ğŸ” ×”××œ×¦×•×ª ×œ××•×“×œ

1. **×¤×™×¦'×¨×™× ×—×©×•×‘×™×**: ×”×ª××§×“ ×‘-{', '.join(target_corr.index[1:4])}
2. **Feature Engineering**:
   - ×¦×•×¨ ×™×—×¡ ×©×œ ×—×“×¨×™× ×œ×’×•×“×œ (Size_sqm)
   - ×¦×•×¨ ××©×ª× ×” ×’×™××•×’×¨×¤×™ ××©×•×œ×‘ (Latitude, Longitude)
   - ×¦×•×¨ ××©×ª× ×” ××¨×—×§ ××©×•×œ×‘ (DistanceSea_km, DistanceCenter_km)
3. **Outliers**: ×©×§×•×œ ×œ×”×¡×™×¨ ××• ×œ×˜×¤×œ ×‘-outliers ×—×¨×™×¤×™×
4. **××•×“×œ×™× ××•××œ×¦×™×**:
   - Linear Regression (baseline)
   - Random Forest (×˜×™×¤×•×œ ×˜×•×‘ ×‘-non-linearity)
   - Gradient Boosting (×‘×™×¦×•×¢×™× ×’×‘×•×”×™×)

---
*× ×•×¦×¨ ××•×˜×•××˜×™×ª ×‘-{datetime.now().strftime("%Y-%m-%d %H:%M")}*
"""

            # ×©××™×¨×ª ×”×ª×•×‘× ×•×ª
            insights_path = os.path.join(output_dir, "insights.md")
            with open(insights_path, 'w', encoding='utf-8') as f:
                f.write(insights)

            return "âœ“ ×ª×•×‘× ×•×ª × ×•×¦×¨×• ×•× ×©××¨×• ×‘-insights.md"

        except Exception as e:
            return f"âŒ ×©×’×™××”: {str(e)}"


class EDATools:
    """××—×œ×§×” ×”××›×™×œ×” ××ª ×›×œ ×›×œ×™ ×”-EDA"""

    @staticmethod
    def get_tools():
        """××—×–×™×¨×” ×¨×©×™××” ×©×œ ×›×œ ×”×›×œ×™×"""
        return [
            DistributionAnalysisTool(),
            CorrelationAnalysisTool(),
            GeographicAnalysisTool(),
            InsightsGeneratorTool()
        ]
